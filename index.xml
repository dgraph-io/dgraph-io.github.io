<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Dgraph Blog</title>
    <link>https://open.dgraph.io/</link>
    <description>Recent content on Dgraph Blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>Copyright (c) 2016, Dgraph Labs, Inc. All rights reserved.</copyright>
    <lastBuildDate>Thu, 21 Jul 2016 11:53:04 +1000</lastBuildDate>
    <atom:link href="https://open.dgraph.io/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Gru: Open source solution for better technical interviews</title>
      <link>https://open.dgraph.io/post/gru/</link>
      <pubDate>Thu, 21 Jul 2016 11:53:04 +1000</pubDate>
      
      <guid>https://open.dgraph.io/post/gru/</guid>
      <description>

&lt;p&gt;Candidate &lt;strong&gt;REJECTED&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;4 out of 5 interviewers had liked the candidate. I was one of the 4. He had received either above or very close to 3.0, which is a good score. The interviewer who didn&amp;rsquo;t like the candidate had been at Google since early 2004. And he didn&amp;rsquo;t like the candidate&amp;rsquo;s joke question about whether he was very rich because he joined before Google went IPO. &lt;em&gt;I guess he wasn&amp;rsquo;t.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;The system in place was methodical. On a scale of 0.0 to 5.0, only terrible candidates received less than 2.0. And only in very rare cases did someone receives above 4.0. Anything above 3.0 was a pretty solid YES for hire. But how the interviewers scored a candidate was very subjective. There was no standardized training for interviewers on how to interview. Some people tend to give higher scores; some tend to give lower scores. So, the hiring committee had been asked to take the interviewer&amp;rsquo;s previous scoring patterns into account, along with how much evidence and conviction they had presented in the feedback form.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;It was duct tape on a broken system.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;It&amp;rsquo;s not an issue specific to Google. Many tech companies follow some variance of the same approach. Two phone screens + five onsite technical interviews, followed by feedback presented to a committee which then decided if the candidate should be hired or not.&lt;/p&gt;

&lt;p&gt;Except that the interviewers have no particular training in judging other people. Most would ask one question; which would get dragged on for the entire length of the interview. If the candidate did well on that question, they might ask another one. So, if the candidate messes up that first question, there&amp;rsquo;s no coming back, mostly even including the rest of the interviews (unless you do exceptionally well in the rest).&lt;/p&gt;

&lt;p&gt;Also, given the inflow of smart young engineers flowing into the valley, the questions are generic enough to be answered by &lt;em&gt;anyone&lt;/em&gt;. So, these drill down to basic computer science concepts &amp;ndash; graphs, trees, sorting, and other algorithms. The candidate is expected to come up with a solution and code it up on a whiteboard within the allocated time under pressure.&lt;/p&gt;

&lt;p&gt;Not surprisingly, &lt;strong&gt;experienced professionals tend to do worse at these interviews than fresh grads&lt;/strong&gt;. Very few professional tech problem can be coded up in 20 mins. Over years of coding, they lose the practice of solving simple problems under time pressure, instead focusing on deeper, harder design problems.&lt;/p&gt;

&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;Google: 90% of our engineers use the software you wrote (Homebrew), but you can’t invert a binary tree on a whiteboard so fuck off.&lt;/p&gt;&amp;mdash; Max Howell (@mxcl) &lt;a href=&#34;https://twitter.com/mxcl/status/608682016205344768&#34;&gt;June 10, 2015&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;//platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;

&lt;p&gt;I&amp;rsquo;m convinced that the existing technical interview system is expensive and broken.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Without any objective measurement, judging the smarts, skills and experience of an engineer is just handwaving.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Triplebyte recently &lt;a href=&#34;http://blog.triplebyte.com/three-hundred-programming-interviews-in-thirty-days&#34;&gt;came up with a post about hiring&lt;/a&gt;. I found it instantly interesting. With a lot of data, they found a higher correlation between a quiz and successful candidates; and a lower one between coding questions and successful candidates.
This got us thinking.
&lt;strong&gt;If we wanted to design an objective interviewing system, we should design a quiz.&lt;/strong&gt;
It would have multiple benefits:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;A single question wouldn&amp;rsquo;t dictate the entire interview. If the candidate is stuck, they can just skip the question.&lt;/li&gt;
&lt;li&gt;Focus on different aspects of systems and algorithms, with multiple questions from each. Looking at a final report would make it clear where the strengths and weaknesses of the candidate lie.&lt;/li&gt;
&lt;li&gt;Give us an objective score, which we can then use to compare them against other candidates.&lt;/li&gt;
&lt;li&gt;Generate a common repository of interview questions, which can ascertain that any leaked questions can&amp;rsquo;t be asked again, and new ones can be added collectively by engineers.&lt;/li&gt;
&lt;li&gt;Cut down on engineers&amp;rsquo; time spent on interviewing significantly. That in itself is a huge benefit about this system. &lt;strong&gt;Any human interaction can then focus on the cultural fit&lt;/strong&gt;, which I think is equally important to technical skills.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;And we did exactly this. We painstakingly and after much cross-examination came up with 30 or so quiz questions in the house. These questions test various aspects of algorithms, concurrency and server interaction.&lt;/p&gt;

&lt;p&gt;Over the past months, we have had 20 candidates take our quiz. It&amp;rsquo;s a small number, but it&amp;rsquo;s still telling. The following graph shows how the candidates fared.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://open.dgraph.io/images/score.png&#34; alt=&#34;Graph representing scores of different candidates&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Note that the red dots represent the score (y-axis left), and the green dots represent the time taken (y-axis right). Candidates with experience in big companies are tagged as such - GB for Googlers who primarily worked on backend/infrastructure projects, GO for Googlers who worked on other projects, Intel, Microsoft and Amazon. Every other company whose name we didn&amp;rsquo;t recognize (startup/smaller company) is tagged as OC.&lt;/p&gt;

&lt;p&gt;What we can learn from this:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;At Dgraph, we have an affinity for Googlers!&lt;/li&gt;
&lt;li&gt;The performance of Googlers on our quiz varied quite a lot. Googlers with backend experience performed not only great but the best out of all candidates.&lt;/li&gt;
&lt;li&gt;Googlers from other fields didn&amp;rsquo;t fare so well. This goes to show that there&amp;rsquo;s quite a significant difference in skills and experience of Googlers working on different projects.&lt;/li&gt;
&lt;li&gt;Note also that there&amp;rsquo;s a selection bias here because our quiz includes questions about concurrency and distributed systems. Folks with frontend experience don&amp;rsquo;t deal with these problems on a regular basis and hence didn&amp;rsquo;t do so well; which in a way is a validation that the quiz system worked as intended.&lt;/li&gt;
&lt;li&gt;Worst scorers tend to take more time than the best. In fact, all 4 top scorers finished approximately under 50 mins, at least 10 mins before the test ended, because they ran out of questions to answer. In fact, the top scoring candidate finished the quiz in half the time!&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;So, we decided to improve upon this basic quiz and converted it to an adaptive test.&lt;/strong&gt;
Based on Triplebyte&amp;rsquo;s and our experience, we know that best candidates take less time to solve the quiz.
We hypothesize if the best performers were allowed to answer more questions in the same fixed time, their score would have been significantly better.&lt;/p&gt;

&lt;p&gt;So, instead of showing a fixed number of questions to be answered in a given maximum time duration, we fix the time and let the candidates answer as many questions as they can in an hour.
And if they&amp;rsquo;re performing well, we show them harder questions carrying higher scores (and vice-versa).
This system would make the difference between an okay candidate and a stellar candidate much more evident.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;In fact, the best candidates would become outliers.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;This inspiration is what &lt;a href=&#34;https://github.com/dgraph-io/gru&#34;&gt;lead to Gru&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&#34;gru-finding-the-right-minions&#34;&gt;Gru: Finding the right minions&lt;/h2&gt;

&lt;p&gt;Gru is an open-source, adaptive quiz system written entirely in Go. Gru server has a simple design and is very easy to setup. It doesn&amp;rsquo;t require maintaining a database. Questions and candidate information are stored and read from files. All the communication between the client and the server is encrypted.&lt;/p&gt;

&lt;p&gt;Engineers collectively come up with questions from different areas and put them in a YAML formatted file. They tag them as &lt;code&gt;easy&lt;/code&gt;, &lt;code&gt;medium&lt;/code&gt;, &lt;code&gt;hard&lt;/code&gt;. You can add more tags to define the field, for, e.g., &lt;code&gt;concurrency&lt;/code&gt;, &lt;code&gt;graph&lt;/code&gt;, &lt;code&gt;sorting&lt;/code&gt;, &lt;code&gt;searching&lt;/code&gt;, etc. Each question has it&amp;rsquo;s own positive and negative scores, generally determined based on their complexity, uniqueness, or statistical probability of anyone getting it right, etc. Here&amp;rsquo;s an example from our demo quiz.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;- id: spacecraftmars
  str: Which year did we first land a spacecraft on Mars?
  correct: [spacecraftmars-1976]
  opt:
  - uid: spacecraftmars-2001
    str: 2001
  - uid: spacecraftmars-1976
    str: 1976
  - uid: spacecraftmars-1981
    str: 1981
  - uid: spacecraftmars-2013
    str: 2013
  positive: 5
  negative: 2.5
  tags: [medium,demo,mars]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Gru client runs on the command line and makes use of &lt;a href=&#34;https://github.com/gizak/termui&#34;&gt;termui&lt;/a&gt; for displaying the questions. The score of the candidate is always visible to them, providing real-time feedback on how they are doing during the test.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://open.dgraph.io/images/gru_screenshot.png&#34; alt=&#34;Screenshot of the Gru client in action&#34; /&gt;&lt;/p&gt;

&lt;p&gt;If you want to get your hands dirty, we have a server running with some demo questions. The binaries for both Gru server and Gru client &lt;a href=&#34;https://github.com/dgraph-io/gru/releases&#34;&gt;are released here&lt;/a&gt;. To take the demo quiz, download the gruclient binary for your platform and just run it. It would automatically connect to our Gru server, and test your knowledge about humankind&amp;rsquo;s space missions.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;You could start running Gru at your company for free. For more details on how Gru works and how to host it, please visit &lt;a href=&#34;https://wiki.dgraph.io/Gru&#34;&gt;Gru wiki&lt;/a&gt;.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Gru is a work in progress. Let us know what you think about it and how we could improve it on &lt;a href=&#34;https://discuss.dgraph.io&#34;&gt;discuss.dgraph.io&lt;/a&gt;. If you find any issues with Gru, &lt;a href=&#34;https://github.com/dgraph-io/gru/issues&#34;&gt;file a bug&lt;/a&gt;. Hope you like Gru and looking forward to a world with better technical interviews.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Gru Links
- &lt;a href=&#34;https://github.com/dgraph-io/gru&#34;&gt;Github repository&lt;/a&gt;
- &lt;a href=&#34;https://wiki.dgraph.io/Gru&#34;&gt;Gru Wiki&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Other posts I&amp;rsquo;ve written about interview process:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://medium.com/@manishrjain/technical-interviews-open-ended-design-questions-ea7fff3486a7#.d6y5gyjys&#34;&gt;Technical Interviews&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.quora.com/Why-cant-I-get-a-job-at-some-tech-giants-despite-doing-well-at-interviews/answer/Manish-Rai-Jain?srid=5tVr&#34;&gt;Why can&amp;rsquo;t I get a job despite doing well in interviews?&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;div class=&#34;engage&#34;&gt;
  &lt;p&gt;We are building an open source, scalable and distributed graph database.&lt;/p&gt;
  &lt;table&gt;
    &lt;thead&gt;
    &lt;tr&gt;
    &lt;th&gt;&lt;/th&gt;
    &lt;th&gt;&lt;/th&gt;
    &lt;/tr&gt;
    &lt;/thead&gt;
    &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;See our live demo.&lt;/td&gt;
      &lt;td&gt;&lt;a href=&#34;https://dgraph.io&#34; target=&#34;_blank&#34;&gt;http://dgraph.io&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;We are hiring. Join us!&lt;/td&gt;
      &lt;td&gt;&lt;a href=&#34;https://dgraph.io#jobs&#34; target=&#34;_blank&#34;&gt;http://dgraph.io#jobs&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Star us on Github.&lt;/td&gt;
      &lt;td&gt;&lt;a href=&#34;https://github.com/dgraph-io/dgraph&#34; target=&#34;_blank&#34;&gt;https://github.com/dgraph-io/dgraph&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Ask us questions.&lt;/td&gt;
      &lt;td&gt;&lt;a href=&#34;https://discuss.dgraph.io&#34; target=&#34;_blank&#34;&gt;https://discuss.dgraph.io&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;/tbody&gt;
  &lt;/table&gt;
&lt;/div&gt;

</description>
    </item>
    
    <item>
      <title>Releasing v0.4</title>
      <link>https://open.dgraph.io/post/v0.4-release/</link>
      <pubDate>Thu, 14 Jul 2016 16:16:14 +1000</pubDate>
      
      <guid>https://open.dgraph.io/post/v0.4-release/</guid>
      <description>

&lt;p&gt;Thanks for your feedback over the last couple of months. This release addresses some of the main pain points of using Dgraph.&lt;/p&gt;

&lt;h2 id=&#34;installation&#34;&gt;Installation&lt;/h2&gt;

&lt;p&gt;Dgraph uses &lt;a href=&#34;http://rocksdb.org/&#34;&gt;RocksDB&lt;/a&gt;, which doesn&amp;rsquo;t have a native Go interface.
To use RocksDB via Go, Cgo is required, which makes doing a simple &lt;code&gt;go get&lt;/code&gt; installation of Dgraph hard.
You first need to download, compile and install RocksDB, before installing Dgraph.
While &lt;code&gt;go get&lt;/code&gt; would still require more manual steps, starting this version only contributors to Dgraph would have to do that.&lt;/p&gt;

&lt;p&gt;We have embedded RocksDB into Dgraph binary and generated binaries for both Linux and Mac.
Now, most users can just choose one of these two options:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ curl https://get.dgraph.io | bash
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Or, download the latest release &lt;a href=&#34;https://github.com/dgraph-io/dgraph/releases&#34;&gt;from Github&lt;/a&gt;. And extract the binaries to &lt;code&gt;/usr/local/bin&lt;/code&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# For Linux
$ sudo tar -C /usr/local/bin -xzf dgraph-linux-amd64-VERSION.tar.gz

# For Mac
$ sudo tar -C /usr/local/bin -xzf dgraph-darwin-amd64-VERSION.tar.gz
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;That&amp;rsquo;s it! These simple steps give you everything that you need to install Dgraph.&lt;/p&gt;

&lt;h2 id=&#34;documentation&#34;&gt;Documentation&lt;/h2&gt;

&lt;p&gt;Dgraph is a small team of enthusiastic developers, who like to code more than they like to write docs.
Even when the team writes documentation, it gets stale very quickly.
Honestly, that&amp;rsquo;s not a story unique to us. It&amp;rsquo;s a common problem affecting many projects.&lt;/p&gt;

&lt;p&gt;So, I thought why not solve it in a way where our documentation would never get stale.
I looked around for inspiration and found these two documentations that maximized screen estate usage and were easy to follow.
Each of these allowed search engines to easily index them while providing a lot of content per web page, which made doing a browser search easy.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://wiki.archlinux.org/&#34;&gt;Arch Linux&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://wiki.gentoo.org/wiki/Main_Page&#34;&gt;Gentoo Linux&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;I run Arch Linux on my desktop and laptop, and love it&amp;rsquo;s thorough up to date documentation.
So, I couldn&amp;rsquo;t be more excited about building something similar for Dgraph.&lt;/p&gt;

&lt;p&gt;So, after last 3 weeks of &lt;em&gt;no-coding-only-documentation&lt;/em&gt; effort, &lt;strong&gt;presenting &lt;a href=&#34;https://wiki.dgraph.io&#34;&gt;wiki.dgraph.io&lt;/a&gt;, the official source of Dgraph documentation&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;Just like Arch and Gentoo, it&amp;rsquo;s using MediaWiki software, which most people use on a daily basis (Thanks, Wikipedia).&lt;/p&gt;

&lt;p&gt;The main reason we chose wiki over other solutions is that it puts the power of editing right in the hands of the user.
So if a user spots an error, they have everything they need to fix it.
If the fix is more complicated, writing a note stating that this article or section is out of date is sufficient to get the team and more importantly, other users notified; so they can take appropriate action.&lt;/p&gt;

&lt;p&gt;And Dgraph community welcomed the change! We already have contributions to wiki from community members on various pages.
They keep us accountable and help maintain the quality of the documentation.
So, check it out, and if you find any issues or need more clarity in some section, log in, and edit the page. It&amp;rsquo;s that simple.&lt;/p&gt;

&lt;p&gt;Btw, if you installed Dgraph above, this is the page you&amp;rsquo;re looking for: &lt;a href=&#34;https://wiki.dgraph.io/Beginners_Guide&#34;&gt;https://wiki.dgraph.io/Beginners_Guide&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&#34;clients&#34;&gt;Clients&lt;/h2&gt;

&lt;p&gt;In v0.3, we released a Go client for Dgraph.
Thanks to our enthusiastic community, we got contributions for other languages. Now we have client code for both &lt;strong&gt;Java&lt;/strong&gt; and &lt;strong&gt;Python&lt;/strong&gt;.
Thanks &lt;a href=&#34;https://github.com/bitmalloc&#34;&gt;@bitmalloc&lt;/a&gt; and &lt;a href=&#34;https://github.com/mohitranka&#34;&gt;@mohitranka&lt;/a&gt;!&lt;/p&gt;

&lt;p&gt;In fact, Python client installation is as simple as:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ pip install -U pydgraph
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;You can read more about how to install and use them here: &lt;a href=&#34;https://wiki.dgraph.io/Clients&#34;&gt;https://wiki.dgraph.io/Clients&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&#34;debugging&#34;&gt;Debugging&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Why did this query take so long to run?&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;This question stumped us! We didn&amp;rsquo;t know why. &lt;code&gt;logrus&lt;/code&gt; was useful only for general logging.
It couldn&amp;rsquo;t pinpoint which log was from which query, and show which steps of execution took how long, during the life of the query.&lt;/p&gt;

&lt;p&gt;The moment I realized that &lt;a href=&#34;https://grpc.io&#34;&gt;grpc.io&lt;/a&gt; is a ground-up rewrite of stubby, Google&amp;rsquo;s internal rpc system; I knew that we had to switch to it.
Stubby could provide amazing debugging information about each request to the server, and something that&amp;rsquo;s very well translated to grpc.&lt;/p&gt;

&lt;p&gt;So, after thorough effort, I was able to replace most of the &lt;code&gt;logrus.Log&lt;/code&gt; statements to use &lt;a href=&#34;https://godoc.org/golang.org/x/net/context&#34;&gt;context&lt;/a&gt; and &lt;a href=&#34;https://godoc.org/golang.org/x/net/trace&#34;&gt;trace&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Thanks to these, you can now easily debug live queries running on Dgraph.
In fact, you can see all this in action on one of our demo Dgraph instance &lt;a href=&#34;http://dgraph.xyz/debug/requests?fam=Dgraph&amp;amp;b=0&amp;amp;exp=1&#34;&gt;via /debug/requests&lt;/a&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;2016/07/14 06:54:55.615143    0.003184    Query
06:54:55.615157     .    14    ... Query received: { me(_xid_: m.0bxtg) { type.object.name.en film.actor.film { film.performance.film { type.object.name.en type.object.name.ru } } } }
06:54:55.615295     .   138    ... Xid: m.0bxtg Uid: 14685953405111677952
06:54:55.615312     .    17    ... Query parsed
06:54:55.615323     .    11    ... Sample value for attr: _root_ Val:
06:54:55.617188     .    55    ... (18 events discarded)
06:54:55.617190     .     3    ... Reply from child. Index: 1 Attr: type.object.name.ru
06:54:55.617192     .     2    ... Reply from child. Index: 0 Attr: film.performance.film
06:54:55.617194     .     2    ... Reply from child. Index: 1 Attr: film.actor.film
06:54:55.617195     .     1    ... Graph processed
06:54:55.618252     .  1057    ... Latencies: Total: 3.102143ms Parsing: 163.283µs Process: 1.88333ms Json: 601.266µs
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;These live queries are further categorized by how long they took to run. So, you can easily identify slower queries.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;2016/07/14 03:16:13.684030    0.100109    Query
03:16:13.684045     .    15    ... Query received: { me(_xid_: m.06pj8) { type.object.name.en film.director.film { type.object.name.en film.film.initial_release_date film.film.country film.film.starring { film.performance.actor { type.object.name.en } film.performance.character { type.object.name.en } } film.film.genre { type.object.name.en } } } }
03:16:13.684175     .   130    ... Xid: m.06pj8 Uid: 4255310415198890869
03:16:13.684193     .    17    ... Query parsed
03:16:13.684197     .     4    ... Sample value for attr: _root_ Val:
03:16:13.744452     .    41    ... (47 events discarded)
03:16:13.744458     .     5    ... Reply from child. Index: 1 Attr: film.performance.character
03:16:13.744466     .     8    ... Reply from child. Index: 4 Attr: film.film.genre
03:16:13.744477     .    11    ... Reply from child. Index: 1 Attr: film.director.film
03:16:13.744482     .     5    ... Graph processed
03:16:13.783501     . 39020    ... Latencies: Total: 99.461379ms Parsing: 156.916µs Process: 60.288205ms Json: 25.813948ms
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Also, you can see latency (in microseconds) statistics over various time intervals.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Count: 20    Mean: 59979    StdDev: 37593    Median: 84261
[    2048,    4096)    4    20.000%    20.000%
[    4096,    8192)    2    10.000%    30.000%
[    65536,    131072)    14    70.000%    100.000%
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This is the first time we have such data available, and we&amp;rsquo;ll tweak what gets reported over time to make these logs more and more useful.&lt;/p&gt;

&lt;p&gt;Hope you like these features and try out &lt;a href=&#34;https://github.com/dgraph-io/dgraph/releases&#34;&gt;the new release&lt;/a&gt;.&lt;/p&gt;

&lt;hr /&gt;

&lt;div class=&#34;engage&#34;&gt;
  &lt;p&gt;We are building an open source, scalable and distributed graph database.&lt;/p&gt;
  &lt;table&gt;
    &lt;thead&gt;
    &lt;tr&gt;
    &lt;th&gt;&lt;/th&gt;
    &lt;th&gt;&lt;/th&gt;
    &lt;/tr&gt;
    &lt;/thead&gt;
    &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;See our live demo.&lt;/td&gt;
      &lt;td&gt;&lt;a href=&#34;https://dgraph.io&#34; target=&#34;_blank&#34;&gt;http://dgraph.io&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;We are hiring. Join us!&lt;/td&gt;
      &lt;td&gt;&lt;a href=&#34;https://dgraph.io#jobs&#34; target=&#34;_blank&#34;&gt;http://dgraph.io#jobs&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Star us on Github.&lt;/td&gt;
      &lt;td&gt;&lt;a href=&#34;https://github.com/dgraph-io/dgraph&#34; target=&#34;_blank&#34;&gt;https://github.com/dgraph-io/dgraph&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Ask us questions.&lt;/td&gt;
      &lt;td&gt;&lt;a href=&#34;https://discuss.dgraph.io&#34; target=&#34;_blank&#34;&gt;https://discuss.dgraph.io&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;/tbody&gt;
  &lt;/table&gt;
&lt;/div&gt;


&lt;p&gt;&lt;em&gt;Thanks &lt;a href=&#34;https://twitter.com/mholt6&#34;&gt;Matt Holt&lt;/a&gt; from the Caddy team, for the inspiration to have &lt;code&gt;https://get.dgraph.io&lt;/code&gt;.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Top image: &lt;a href=&#34;https://www.nasa.gov/image-feature/cygnus-cargo-craft-released-from-space-station/&#34;&gt;Cygnus Cargo Craft Released From Space Station&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Custom encoding: Go implementation in net/rpc vs grpc and why we switched</title>
      <link>https://open.dgraph.io/post/rpc-vs-grpc/</link>
      <pubDate>Sat, 25 Jun 2016 19:06:45 +1000</pubDate>
      
      <guid>https://open.dgraph.io/post/rpc-vs-grpc/</guid>
      <description>

&lt;p&gt;At Dgraph, we aim to build a low latency, distributed graph database.
This means our data is distributed among nodes in the cluster.
Executing a query means multiple nodes are communicating with each other.
To keep our latency of communication low, we use a new form of serialization library called &lt;a href=&#34;https://google.github.io/flatbuffers/&#34;&gt;Flatbuffers&lt;/a&gt;.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;What sets FlatBuffers apart is that it represents hierarchical data in a flat binary buffer in such a way that it can still be accessed directly without parsing/unpacking, while also still supporting data structure evolution (forwards/backwards compatibility).
The only memory needed to access your data is that of the buffer.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;strong&gt;How is Flatbuffers better than &lt;a href=&#34;https://developers.google.com/protocol-buffers/&#34;&gt;Protocol Buffers&lt;/a&gt;&lt;/strong&gt;? FlatBuffers does not need a parsing/unpacking step to a secondary representation before you can access data, often coupled with per-object memory allocation.&lt;/p&gt;

&lt;p&gt;Dgraph responses can contain millions of entities and binary blob values.
And the fact that Flatbuffers doesn&amp;rsquo;t need to recreate the entire information in language specific data structures is very helpful for both memory and speed.&lt;/p&gt;

&lt;p&gt;Also, TCP is always going to be faster than HTTP, because HTTP is one extra layer on top of TCP.
So, our goal was to implement communication using RPC over custom encoding utilizing Flatbuffers.&lt;/p&gt;

&lt;h1 id=&#34;go-net-rpc&#34;&gt;Go net/rpc&lt;/h1&gt;

&lt;p&gt;Our first approach was to use Go language library &lt;code&gt;net/rpc&lt;/code&gt; and implement custom encoding in it.&lt;/p&gt;

&lt;p&gt;Helper function to deal with writing and parsing header for the payload:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;/*
 * Copyright 2016 DGraph Labs, Inc.
 *
 * Licensed under the Apache License, Version 2.0 (the &amp;quot;License&amp;quot;);
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 *         http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an &amp;quot;AS IS&amp;quot; BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package conn

import (
    &amp;quot;bytes&amp;quot;
    &amp;quot;encoding/binary&amp;quot;
    &amp;quot;fmt&amp;quot;
    &amp;quot;io&amp;quot;

    &amp;quot;github.com/dgraph-io/dgraph/x&amp;quot;
)

type Query struct {
    Data []byte
}

type Reply struct {
    Data []byte
}

func writeHeader(rwc io.ReadWriteCloser, seq uint64,
    method string, data []byte) error {

    var bh bytes.Buffer
    var rerr error

    // In package x: func SetError(prev *error, n error)
    x.SetError(&amp;amp;rerr, binary.Write(&amp;amp;bh, binary.LittleEndian, seq))
    x.SetError(&amp;amp;rerr, binary.Write(&amp;amp;bh, binary.LittleEndian, int32(len(method))))
    x.SetError(&amp;amp;rerr, binary.Write(&amp;amp;bh, binary.LittleEndian, int32(len(data))))
    _, err := bh.Write([]byte(method))
    x.SetError(&amp;amp;rerr, err)
    if rerr != nil {
        return rerr
    }
    _, err = rwc.Write(bh.Bytes())
    return err
}

func parseHeader(rwc io.ReadWriteCloser, seq *uint64,
    method *string, plen *int32) error {

    var err error
    var sz int32
    x.SetError(&amp;amp;err, binary.Read(rwc, binary.LittleEndian, seq))
    x.SetError(&amp;amp;err, binary.Read(rwc, binary.LittleEndian, &amp;amp;sz))
    x.SetError(&amp;amp;err, binary.Read(rwc, binary.LittleEndian, plen))
    if err != nil {
        return err
    }

    buf := make([]byte, sz)
    n, err := rwc.Read(buf)
    if err != nil {
        return err
    }
    if n != int(sz) {
        return fmt.Errorf(&amp;quot;Expected: %v. Got: %v\n&amp;quot;, sz, n)
    }
    *method = string(buf)
    return nil
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Code at server to read requests and write responses:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;/*
 * Copyright 2016 DGraph Labs, Inc.
 *
 * Licensed under the Apache License, Version 2.0 (the &amp;quot;License&amp;quot;);
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 *         http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an &amp;quot;AS IS&amp;quot; BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package conn

import (
    &amp;quot;io&amp;quot;
    &amp;quot;log&amp;quot;
    &amp;quot;net/rpc&amp;quot;
)

type ServerCodec struct {
    Rwc        io.ReadWriteCloser
    payloadLen int32
}

func (c *ServerCodec) ReadRequestHeader(r *rpc.Request) error {
    return parseHeader(c.Rwc, &amp;amp;r.Seq, &amp;amp;r.ServiceMethod, &amp;amp;c.payloadLen)
}

func (c *ServerCodec) ReadRequestBody(data interface{}) error {
    b := make([]byte, c.payloadLen)
    _, err := io.ReadFull(c.Rwc, b)
    if err != nil {
        return err
    }

    if data == nil {
        // If data is nil, discard this request.
        return nil
    }
    query := data.(*Query)
    query.Data = b
    return nil
}

func (c *ServerCodec) WriteResponse(resp *rpc.Response,
    data interface{}) error {

    if len(resp.Error) &amp;gt; 0 {
        log.Fatal(&amp;quot;Response has error: &amp;quot; + resp.Error)
    }
    if data == nil {
        log.Fatal(&amp;quot;Worker write response data is nil&amp;quot;)
    }
    reply, ok := data.(*Reply)
    if !ok {
        log.Fatal(&amp;quot;Unable to convert to reply&amp;quot;)
    }

    if err := writeHeader(c.Rwc, resp.Seq,
        resp.ServiceMethod, reply.Data); err != nil {
        return err
    }

    _, err := c.Rwc.Write(reply.Data)
    return err
}

func (c *ServerCodec) Close() error {
    return c.Rwc.Close()
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Similarly, the code at the client to read requests and write responses:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;/*
 * Copyright 2016 DGraph Labs, Inc.
 *
 * Licensed under the Apache License, Version 2.0 (the &amp;quot;License&amp;quot;);
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 *         http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an &amp;quot;AS IS&amp;quot; BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package conn

import (
    &amp;quot;errors&amp;quot;
    &amp;quot;fmt&amp;quot;
    &amp;quot;io&amp;quot;
    &amp;quot;log&amp;quot;
    &amp;quot;net/rpc&amp;quot;
)

type ClientCodec struct {
    Rwc        io.ReadWriteCloser
    payloadLen int32
}

func (c *ClientCodec) WriteRequest(r *rpc.Request, body interface{}) error {
    if body == nil {
        return fmt.Errorf(&amp;quot;Nil request body from client.&amp;quot;)
    }

    query := body.(*Query)
    if err := writeHeader(c.Rwc, r.Seq, r.ServiceMethod, query.Data); err != nil {
        return err
    }
    n, err := c.Rwc.Write(query.Data)
    if n != len(query.Data) {
        return errors.New(&amp;quot;Unable to write payload.&amp;quot;)
    }
    return err
}

func (c *ClientCodec) ReadResponseHeader(r *rpc.Response) error {
    if len(r.Error) &amp;gt; 0 {
        log.Fatal(&amp;quot;client got response error: &amp;quot; + r.Error)
    }
    if err := parseHeader(c.Rwc, &amp;amp;r.Seq,
        &amp;amp;r.ServiceMethod, &amp;amp;c.payloadLen); err != nil {
        return err
    }
    return nil
}

func (c *ClientCodec) ReadResponseBody(body interface{}) error {
    buf := make([]byte, c.payloadLen)
    _, err := io.ReadFull(c.Rwc, buf)
    reply := body.(*Reply)
    reply.Data = buf
    return err
}

func (c *ClientCodec) Close() error {
    return c.Rwc.Close()
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Also, each server should be able to send multiple requests in parallel.
So, we built a connection pool to create, store and reuse multiple connections:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;/*
 * Copyright 2016 DGraph Labs, Inc.
 *
 * Licensed under the Apache License, Version 2.0 (the &amp;quot;License&amp;quot;);
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 *         http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an &amp;quot;AS IS&amp;quot; BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package conn

import (
    &amp;quot;net&amp;quot;
    &amp;quot;net/rpc&amp;quot;
    &amp;quot;strings&amp;quot;
    &amp;quot;time&amp;quot;

    &amp;quot;github.com/dgraph-io/dgraph/x&amp;quot;
)

var glog = x.Log(&amp;quot;conn&amp;quot;) // In package x: func Log(p string) *logrus.Entry

type Pool struct {
    clients chan *rpc.Client
    Addr    string
}

func NewPool(addr string, maxCap int) *Pool {
    p := new(Pool)
    p.Addr = addr
    p.clients = make(chan *rpc.Client, maxCap)
    client, err := p.dialNew()
    if err != nil {
        glog.Fatal(err)
        return nil
    }
    p.clients &amp;lt;- client
    return p
}

func (p *Pool) dialNew() (*rpc.Client, error) {
    d := &amp;amp;net.Dialer{
        Timeout: 3 * time.Minute,
    }
    var nconn net.Conn
    var err error
    // This loop will retry for 10 minutes before giving up.
    for i := 0; i &amp;lt; 60; i++ {
        nconn, err = d.Dial(&amp;quot;tcp&amp;quot;, p.Addr)
        if err == nil {
            break
        }
        if !strings.Contains(err.Error(), &amp;quot;refused&amp;quot;) {
            break
        }

        glog.WithField(&amp;quot;error&amp;quot;, err).WithField(&amp;quot;addr&amp;quot;, p.Addr).
            Info(&amp;quot;Retrying connection...&amp;quot;)
        time.Sleep(10 * time.Second)
    }
    if err != nil {
        return nil, err
    }
    cc := &amp;amp;ClientCodec{
        Rwc: nconn,
    }
    return rpc.NewClientWithCodec(cc), nil
}

func (p *Pool) Call(serviceMethod string, args interface{},
    reply interface{}) error {

    client, err := p.get()
    if err != nil {
        return err
    }
    if err = client.Call(serviceMethod, args, reply); err != nil {
        return err
    }

    select {
    case p.clients &amp;lt;- client:
        return nil
    default:
        return client.Close()
    }
}

func (p *Pool) get() (*rpc.Client, error) {
    select {
    case client := &amp;lt;-p.clients:
        return client, nil
    default:
        return p.dialNew()
    }
}

func (p *Pool) Close() error {
    // We&#39;re not doing a clean exit here. A clean exit here would require
    // synchronization, which seems unnecessary for now. But, we should
    // add one if required later.
    return nil
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This worked well.
And both v0.2 and v0.3 of Dgraph were using this code for the nodes to communicate with each other.&lt;/p&gt;

&lt;hr /&gt;

&lt;h1 id=&#34;the-switch&#34;&gt;The Switch&lt;/h1&gt;

&lt;p&gt;&lt;img src=&#34;https://open.dgraph.io/images/martian-receiving.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;At Dgraph, we spend Fridays learning and improving.
This means reading books, papers, articles, watching talks.
And we came across a great talk by Jeff Dean of Google: &lt;a href=&#34;https://discuss.dgraph.io/t/jeff-deans-talk-about-rapid-response-times/83&#34;&gt;Rapid Response Times&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;As I mentioned above, we care a lot about query latency.
After watching it a couple of times from two different conferences, the prime learning I gathered from his talk was:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Send request to the first replica, telling it that it&amp;rsquo;s going to send it to a second one.&lt;/li&gt;
&lt;li&gt;2 ms later, send the request to the second one, telling it that it&amp;rsquo;s already sent to the first one.&lt;/li&gt;
&lt;li&gt;When one of them starts processing the request, it sends a cancellation request directly to its peer.&lt;/li&gt;
&lt;li&gt;If the peer hasn&amp;rsquo;t started processing the request, it would just cancel the request.&lt;/li&gt;
&lt;li&gt;In a rare case, both of them process it and overall do twice the work.&lt;/li&gt;
&lt;li&gt;Overall, your latencies improve considerably due to this method.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Jeff_Dean_(computer_scientist)&#34;&gt;Jeff Dean&lt;/a&gt; has an impressive track record at Google. He&amp;rsquo;s behind almost every distributed system in production at Google.
So, when he gives a suggestion, you take it seriously.&lt;/p&gt;

&lt;p&gt;At v0.4, we&amp;rsquo;re not doing replication yet. So, we can&amp;rsquo;t send queries to multiple servers in parallel.
However, that&amp;rsquo;s how the system is going to look like a few minor releases down the lane.&lt;/p&gt;

&lt;p&gt;So, we started thinking about how we could change our custom encoding based RPC implementation to achieve something like this.
Around the same time, we were looking for a way to figure out slow rpcs on servers.
&lt;a href=&#34;https://forum.golangbridge.org/t/equivalent-of-rpcz-at-google/2609&#34;&gt;Dave Cheney&amp;rsquo;s response&lt;/a&gt; pointed us to &lt;a href=&#34;http://www.grpc.io/&#34;&gt;grpc.io&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;While I had considered Google built &lt;code&gt;grpc&lt;/code&gt; in the past, I&amp;rsquo;d rejected it understanding that it requires you to use Protocol Buffers; but we&amp;rsquo;d already chosen to go with Flatbuffers.
But when &lt;a href=&#34;https://www.youtube.com/watch?v=sZx3oZt7LVg&#34;&gt;Sameer Ajmani&amp;rsquo;s talk&lt;/a&gt; pointed that &lt;code&gt;grpc&lt;/code&gt; is essentially a rewrite of Google internal Stubby from ground up, that got me to dig deeper.
&lt;code&gt;grpc&lt;/code&gt; came with &lt;code&gt;net/context&lt;/code&gt; which could easily do what Jeff Dean had talked about.
Also, it can help see live rpcs and track the slowest ones.&lt;/p&gt;

&lt;p&gt;Overall, there was a lot of advantages to switching to &lt;code&gt;grpc&lt;/code&gt;.
But, we didn&amp;rsquo;t want to give up the performance benefits of Flatbuffers.&lt;/p&gt;

&lt;p&gt;So, digging deeper, we found that &lt;code&gt;grpc&lt;/code&gt; did support custom encoding. And we implemented it.
This is the whole equivalent code implemented in &lt;code&gt;grpc&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;/*
 * Copyright 2016 DGraph Labs, Inc.
 *
 * Licensed under the Apache License, Version 2.0 (the &amp;quot;License&amp;quot;);
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 *         http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an &amp;quot;AS IS&amp;quot; BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package worker

import (
    &amp;quot;log&amp;quot;

    &amp;quot;google.golang.org/grpc&amp;quot;
)

type PayloadCodec struct{}

func (cb *PayloadCodec) Marshal(v interface{}) ([]byte, error) {
    p, ok := v.(*Payload)
    if !ok {
        log.Fatalf(&amp;quot;Invalid type of struct: %+v&amp;quot;, v)
    }
    return p.Data, nil
}

func (cb *PayloadCodec) Unmarshal(data []byte, v interface{}) error {
    p, ok := v.(*Payload)
    if !ok {
        log.Fatalf(&amp;quot;Invalid type of struct: %+v&amp;quot;, v)
    }
    p.Data = data
    return nil
}

func (cb *PayloadCodec) String() string {
    return &amp;quot;worker.PayloadCodec&amp;quot;
}

type Pool struct {
    conns chan *grpc.ClientConn
    Addr  string
}

func NewPool(addr string, maxCap int) *Pool {
    p := new(Pool)
    p.Addr = addr
    p.conns = make(chan *grpc.ClientConn, maxCap)
    conn, err := p.dialNew()
    if err != nil {
        glog.Fatal(err)
        return nil
    }
    p.conns &amp;lt;- conn
    return p
}

func (p *Pool) dialNew() (*grpc.ClientConn, error) {
    return grpc.Dial(p.Addr, grpc.WithInsecure(), grpc.WithInsecure(),
        grpc.WithCodec(&amp;amp;PayloadCodec{}))
}

func (p *Pool) Get() (*grpc.ClientConn, error) {
    select {
    case conn := &amp;lt;-p.conns:
        return conn, nil
    default:
        return p.dialNew()
    }
}

func (p *Pool) Put(conn *grpc.ClientConn) error {
    select {
    case p.conns &amp;lt;- conn:
        return nil
    default:
        return conn.Close()
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;And this is the proto file with Payload and &lt;code&gt;service&lt;/code&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;syntax = &amp;quot;proto3&amp;quot;;

package worker;

message Payload {
    bytes Data = 1;
}

service Worker {
    rpc Hello (Payload) returns (Payload) {}
    rpc GetOrAssign (Payload) returns (Payload) {}
    rpc Mutate (Payload) returns (Payload) {}
    rpc ServeTask (Payload) returns (Payload) {}
}
&lt;/code&gt;&lt;/pre&gt;

&lt;hr /&gt;

&lt;h1 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h1&gt;

&lt;p&gt;So, turns out, &lt;code&gt;grpc&lt;/code&gt; not only does custom encoding, but it also leads to&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;smaller code footprint.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://godoc.org/golang.org/x/net/context&#34;&gt;net/context&lt;/a&gt;, which in turn allows client to cancel pending rpc requests to servers, among many other benefits.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://godoc.org/golang.org/x/net/trace&#34;&gt;net/trace&lt;/a&gt;, which allows tracing of rpcs and long-lived objects.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;For more, read the &lt;a href=&#34;https://github.com/dgraph-io/dgraph/commit/c4629b907702748694712637cbef1fb2c1f15d07&#34;&gt;pull request which made this change&lt;/a&gt; across our code base.
Hope you find this useful.&lt;/p&gt;

&lt;p&gt;Also read:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Research paper on &lt;a href=&#34;https://www.google.com.au/url?sa=t&amp;amp;rct=j&amp;amp;q=&amp;amp;esrc=s&amp;amp;source=web&amp;amp;cd=2&amp;amp;cad=rja&amp;amp;uact=8&amp;amp;ved=0ahUKEwipuJ3rjsPNAhWEopQKHeeZBkMQFggiMAE&amp;amp;url=http%3A%2F%2Fresearch.google.com%2Fpeople%2Fjeff%2FBerkeley-Latency-Mar2012.pdf&amp;amp;usg=AFQjCNGnP9v7v9xN93MM6KPVJ7FcML6LvQ&amp;amp;bvm=bv.125596728,d.dGo&#34;&gt;Rapid Response Times&lt;/a&gt; by Jeff Dean.&lt;/li&gt;
&lt;li&gt;Blog post on &lt;a href=&#34;https://blog.golang.org/context&#34;&gt;Go Concurrency Patterns: Context&lt;/a&gt; by Sameer Ajmani.&lt;/li&gt;
&lt;li&gt;Slides on &lt;a href=&#34;https://talks.golang.org/2014/gotham-context.slide#1&#34;&gt;Cancellation, Context and Plumbing&lt;/a&gt; by Sameer Ajmani.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;em&gt;Images courtesy: &lt;a href=&#34;http://www.foxmovies.com/movies/the-martian&#34;&gt;The Martian&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;div class=&#34;engage&#34;&gt;
  &lt;p&gt;We are building an open source, scalable and distributed graph database.&lt;/p&gt;
  &lt;table&gt;
    &lt;thead&gt;
    &lt;tr&gt;
    &lt;th&gt;&lt;/th&gt;
    &lt;th&gt;&lt;/th&gt;
    &lt;/tr&gt;
    &lt;/thead&gt;
    &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;See our live demo.&lt;/td&gt;
      &lt;td&gt;&lt;a href=&#34;https://dgraph.io&#34; target=&#34;_blank&#34;&gt;http://dgraph.io&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;We are hiring. Join us!&lt;/td&gt;
      &lt;td&gt;&lt;a href=&#34;https://dgraph.io#jobs&#34; target=&#34;_blank&#34;&gt;http://dgraph.io#jobs&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Star us on Github.&lt;/td&gt;
      &lt;td&gt;&lt;a href=&#34;https://github.com/dgraph-io/dgraph&#34; target=&#34;_blank&#34;&gt;https://github.com/dgraph-io/dgraph&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Ask us questions.&lt;/td&gt;
      &lt;td&gt;&lt;a href=&#34;https://discuss.dgraph.io&#34; target=&#34;_blank&#34;&gt;https://discuss.dgraph.io&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;/tbody&gt;
  &lt;/table&gt;
&lt;/div&gt;

</description>
    </item>
    
    <item>
      <title>Can it really scale?</title>
      <link>https://open.dgraph.io/post/performance-throughput-latency/</link>
      <pubDate>Tue, 21 Jun 2016 10:20:32 +0530</pubDate>
      
      <guid>https://open.dgraph.io/post/performance-throughput-latency/</guid>
      <description>

&lt;p&gt;In this post, we’ll look at how Dgraph performs on varying the number of nodes in the cluster, specs of the machine and load on the server to answer the ultimate question: &lt;em&gt;Can it really scale?&lt;/em&gt;&lt;/p&gt;

&lt;h2 id=&#34;the-dataset&#34;&gt;The Dataset&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Freebase&#34;&gt;Freebase&lt;/a&gt; is an online collection of structured data which includes contributions from many sources including individual and user-generated contributions.
Currently, it has &lt;a href=&#34;https://developers.google.com/freebase/&#34;&gt;1.9 Billion RDF N-Triples&lt;/a&gt; worth 250GB of uncompressed data.
On top of that, this dataset is over 95% accurate with a complex and rich real world schema.
It is an ideal data set to test the performance of Dgraph.
We decided not to use the entire data set as it wasn&amp;rsquo;t necessary for our goal here.&lt;/p&gt;

&lt;p&gt;Given our love for movies, we narrowed it down to the film data.
We ran some scripts and filtered in the movie data only.
All the data and scripts are present in &lt;a href=&#34;https://github.com/dgraph-io/benchmarks/tree/master/data&#34;&gt;our benchmarks repository&lt;/a&gt;.
There are two million nodes, which represent directors, actors, films and all the other objects in the database.
Moreover, 21 million edges (including 4M edges for names) are representing the relationships between actors, films, directors and all the other nodes in the database.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Some interesting information about this data:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# film.film --{film.film.starring}--&amp;gt; [mediator] --{film.performance.actor}--&amp;gt; film.actor
# Film --&amp;gt; Mediator
$ zgrep &amp;quot;&amp;lt;film.film.starring&amp;gt;&amp;quot; rdf-films.gz | wc -l
1397647

# Mediator --&amp;gt; Actor
$ zgrep &amp;quot;&amp;lt;film.performance.actor&amp;gt;&amp;quot; rdf-films.gz | wc -l
1396420

# Film --&amp;gt; Director
$ zgrep &amp;quot;&amp;lt;film.film.directed_by&amp;gt;&amp;quot; rdf-films.gz | wc -l
242212

# Director --&amp;gt; Film
$ zgrep &amp;quot;&amp;lt;film.director.film&amp;gt;&amp;quot; rdf-films.gz | wc -l
245274

# Film --&amp;gt; Initial Release Date
$ zgrep &amp;quot;&amp;lt;film.film.initial_release_date&amp;gt;&amp;quot; rdf-films.gz | wc -l
240858

# Film --&amp;gt; Genre
$ zgrep &amp;quot;&amp;lt;film.film.genre&amp;gt;&amp;quot; rdf-films.gz | wc -l
548152

# Genre --&amp;gt; Film
$ zgrep &amp;quot;&amp;lt;film.film_genre.films_in_this_genre&amp;gt;&amp;quot; rdf-films.gz | wc -l
546698

# Generated language names from names freebase rdf data.
$ zcat langnames.gz | awk &#39;{print $1}&#39; | uniq | sort | uniq | wc -l
55

# Total number of countries.
$ zgrep &amp;quot;&amp;lt;film.film.country&amp;gt;&amp;quot; rdf-films.gz | awk &#39;{print $3}&#39; | uniq | sort | uniq | wc -l
304
&lt;/code&gt;&lt;/pre&gt;

&lt;hr /&gt;

&lt;p&gt;This data set contains information about ~480K actors, ~100K directors and ~240K films.
Some example of entries in the dataset are :&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;lt;m.0102j2vq&amp;gt; &amp;lt;film.actor.film&amp;gt; &amp;lt;m.011kyqsq&amp;gt; .
&amp;lt;m.0102xz6t&amp;gt; &amp;lt;film.performance.film&amp;gt; &amp;lt;m.0kv00q&amp;gt; .
&amp;lt;m.050llt&amp;gt; &amp;lt;type.object.name&amp;gt; “Aishwarya Rai Bachchan”@hr .
&amp;lt;m.0bxtg&amp;gt; &amp;lt;type.object.name&amp;gt; “Tom Hanks”@es .
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;terminology&#34;&gt;Terminology&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;Throughput: Number of queries served by the server per second and received by the client&lt;/li&gt;
&lt;li&gt;Latency: Difference between the time when the server received the request and the time it finished processing the request&lt;/li&gt;
&lt;li&gt;95 percentile latency: The worst case latency which 95 percentage of users that query the database face&lt;/li&gt;
&lt;li&gt;50 percentile latency: The worst case latency which half the users that query the database face&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;setup&#34;&gt;Setup&lt;/h2&gt;

&lt;p&gt;All the testing was done on GCE instances. Each machine had 30GB of SSD and at least 7.5 GB of RAM. The number of cores varied depending on the experiments performed.&lt;/p&gt;

&lt;p&gt;The tests were run for 1-minute intervals during which all the parallel connections made requests to the database.
This was repeated ten times and throughput, mean latency, 95th percentile latency, 50th percentile latency were measured.
Note that for user-facing systems, measuring percentile latency is better than mean latency as the average can be skewed by outliers.&lt;/p&gt;

&lt;p&gt;In a multi-node cluster set up, the queries were distributed among each node in a round-robin fashion.
Note that no single machine contains all the data to answer these queries, in a multi-node cluster.
They still have to communicate with each other to respond to the queries.&lt;/p&gt;

&lt;h2 id=&#34;variables&#34;&gt;Variables&lt;/h2&gt;

&lt;p&gt;The parameters that were varied were:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;A number of parallel connections to the database. In Go, this equated to the number of goroutines a client would have. Each goroutine would run in an infinite loop, querying the database via a blocking function.&lt;/li&gt;
&lt;li&gt;Number of cores per server&lt;/li&gt;
&lt;li&gt;Number of servers in the cluster&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;This gave us an idea of what to expect from the system and would help in predicting the configuration required to handle a given load.&lt;/p&gt;

&lt;h2 id=&#34;queries&#34;&gt;Queries&lt;/h2&gt;

&lt;p&gt;We ran broadly 2 categories of queries.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;For each actor (478,936 actors), get their name, the films they acted in, and those films&amp;rsquo; names.&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code&gt;{
  me ( _xid_ : XID ) {
    type.object.name.en
    film.actor.film {
      film.performance.film {
        type.object.name.en
      }
    }
  }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
&lt;li&gt;For each director (90,063 directors), get their name, the films they directed, and names of all the genres of those films.&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code&gt;{
  me ( _xid_ : XID ) {
    type.object.name.en
    film.director.film {
      film.film.genre {
        type.object.name.en
      }
    }
  }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;During each iteration, either an actor or a director category was chosen randomly.
Furthermore, for that category, an actor or director was chosen randomly; their &lt;code&gt;XID&lt;/code&gt; filled in in the query template.&lt;/p&gt;

&lt;h2 id=&#34;performance&#34;&gt;Performance&lt;/h2&gt;

&lt;p&gt;Let us look at some graphs obtained by varying the machine specs and the number of nodes in the cluster under different loads.&lt;/p&gt;

&lt;h3 id=&#34;vary-the-number-of-cores-in-a-single-instance&#34;&gt;Vary the number of cores in a single instance&lt;/h3&gt;

&lt;p&gt;&lt;img src=&#34;https://open.dgraph.io/images/cores_thru.jpg&#34; alt=&#34;Throughput on varying number of cores&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://open.dgraph.io/images/cores_lat_50.jpg&#34; alt=&#34;50 percentile latency on varying number of cores&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://open.dgraph.io/images/cores_lat_95.jpg&#34; alt=&#34;95 percentile latency on varying number of cores&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://open.dgraph.io/images/cores_lat_mean.jpg&#34; alt=&#34;mean latency on varying number of cores&#34; /&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;With the same number of cores, when we increase the number of connections, i.e. load on the system, the throughput as well as the latency increase.&lt;/li&gt;
&lt;li&gt;Throughput increases till some point and then flattens out. This is the point where the computational capacity is being utilized almost fully.&lt;/li&gt;
&lt;li&gt;As expected, the latency increases almost linearly with the number of connections.&lt;/li&gt;
&lt;li&gt;When we increase the number of cores, the latency decreases and the throughput increases.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;vary-number-of-instances&#34;&gt;Vary number of instances&lt;/h3&gt;

&lt;p&gt;&lt;img src=&#34;https://open.dgraph.io/images/topo_thru.jpg&#34; alt=&#34;Throughput on varying number of instances&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://open.dgraph.io/images/dist_lat_50.jpg&#34; alt=&#34;50 percentile latency on varying number of instances&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://open.dgraph.io/images/dist_lat_95.jpg&#34; alt=&#34;95 percentile latency on varying number of instances&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://open.dgraph.io/images/dist_lat_mean.jpg&#34; alt=&#34;mean latency on varying number of instances&#34; /&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;When we increase the number of parallel connections, the throughput increases, but then flattens out. This is the point where the computational capacity is being utilized almost fully.&lt;/li&gt;
&lt;li&gt;The latency increases almost linearly with the number of connections.&lt;/li&gt;
&lt;li&gt;Latency in the case of a single instance is observed to be the equal to (or a bit lower than) that of distributed configurations as the former doesn’t require any network calls. However, as the number of requests/load increase, the cumulative computational power comes into play and overshadows the latency incurred due to network calls. Hence, the latency reduces in the distributed version under higher loads.&lt;/li&gt;
&lt;li&gt;On comparing across the one, two and five node clusters, we can see that the latency, as well as the throughput, are better for configurations with a higher number of nodes, i.e., when there is more computational capacity at disposal. The throughput increases as we have greater computational power and can handle more queries.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;From the above experiments, we can see a relationship between the throughput, latency and the overall computational power of the cluster.
The graphs show that the throughput increases as the computational power increases.
Which can be achieved either by increasing the number of cores on each server or the number of nodes in the cluster.&lt;/p&gt;

&lt;p&gt;The latency increases as the amount of load on the database increases.
However, the rate of the increase differs based on how much computational power we have available.&lt;/p&gt;

&lt;p&gt;This experiment also shows that there is a limit on how much computational power a single node can have, and once we reach that limit, scaling horizontally is the right option.
Not only that, but it also proves that scaling horizontally improves the performance.
Hence, having more replicas, distributing the dataset optimally across machines are some factors which help in improving the throughput and reducing the latency that the users face.&lt;/p&gt;

&lt;p&gt;Based on this experiment, our recommendation for running Dgraph would be:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Use as many cores as possible&lt;/li&gt;
&lt;li&gt;Have the servers geographically close-by so that network latency is reduced&lt;/li&gt;
&lt;li&gt;Distribute the data among servers and query them in a round-robin fashion for greater throughput&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;These might seem pretty obvious recommendations for a distributed system, but &lt;strong&gt;this experiment proves that the underlying design of Dgraph is scalable.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Hope this helps you get a sense of what sort of performance you could expect out of Dgraph!&lt;/p&gt;

&lt;p&gt;&lt;em&gt;This post is derived from my report for B.tech Project on “A Distributed Implementation of the Graph Database System, Dgraph”.
The full report is &lt;a href=&#34;https://www.dropbox.com/s/7h4ytak39r2pdun/Ashwin_Thesis.pdf?dl=0&#34;&gt;available for download here&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;div class=&#34;engage&#34;&gt;
  &lt;p&gt;We are building an open source, scalable and distributed graph database.&lt;/p&gt;
  &lt;table&gt;
    &lt;thead&gt;
    &lt;tr&gt;
    &lt;th&gt;&lt;/th&gt;
    &lt;th&gt;&lt;/th&gt;
    &lt;/tr&gt;
    &lt;/thead&gt;
    &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;See our live demo.&lt;/td&gt;
      &lt;td&gt;&lt;a href=&#34;https://dgraph.io&#34; target=&#34;_blank&#34;&gt;http://dgraph.io&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;We are hiring. Join us!&lt;/td&gt;
      &lt;td&gt;&lt;a href=&#34;https://dgraph.io#jobs&#34; target=&#34;_blank&#34;&gt;http://dgraph.io#jobs&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Star us on Github.&lt;/td&gt;
      &lt;td&gt;&lt;a href=&#34;https://github.com/dgraph-io/dgraph&#34; target=&#34;_blank&#34;&gt;https://github.com/dgraph-io/dgraph&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Ask us questions.&lt;/td&gt;
      &lt;td&gt;&lt;a href=&#34;https://discuss.dgraph.io&#34; target=&#34;_blank&#34;&gt;https://discuss.dgraph.io&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;/tbody&gt;
  &lt;/table&gt;
&lt;/div&gt;


&lt;p&gt;&lt;em&gt;Top image: &lt;a href=&#34;http://mars.nasa.gov/images/PIA14840.jpg&#34;&gt;Mars Rover Landing via Nasa&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Wisemonk: A slackbot to move discussions from Slack to Discourse</title>
      <link>https://open.dgraph.io/post/wisemonk/</link>
      <pubDate>Wed, 15 Jun 2016 11:39:27 +1000</pubDate>
      
      <guid>https://open.dgraph.io/post/wisemonk/</guid>
      <description>

&lt;blockquote&gt;
&lt;p&gt;Then there was the fact that we had so many channels and direct messages and group chats.
It multiplexed my brain and left me in a constant state of anxiety, feeling that I needed to always be on guard.
&lt;em&gt;— &lt;a href=&#34;https://blog.agilebits.com/2016/04/19/curing-our-slack-addiction/&#34;&gt;Dave Teare, Curing Our Slack Addiction&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h1 id=&#34;the-beginning&#34;&gt;The Beginning&lt;/h1&gt;

&lt;p&gt;&lt;em&gt;— Manish Jain&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Sitting in a Japanese restaurant, waiting for my lunch, this is the line that hit me hard.
I’d experienced this exact thing in my previous company.&lt;/p&gt;

&lt;p&gt;Every block of time dedicated to coding was either interrupted by pings on the chat system, or a consistent feeling that you’re missing out.
Away from the desk, phones would start ringing every evening.
And they’d ring some more on the weekends, Sunday evenings in particular.
We were always doing something — the feeling was like being on &lt;em&gt;unpaid pager duty&lt;/em&gt;.
People are talking, and if I’m not around to show my presence, it would mean I’m not working.
But what was getting done, I wasn’t so clear about.&lt;/p&gt;

&lt;p&gt;So, when we started exploring options for both internal and external communication for Dgraph, we wanted to do things differently.
We’d focus on asynchronous communication, so people can take their time before jumping in.
People can reply when they get to it — this might be hours later.
And there’ll be an incentive to modify your reply as more things become clear to you or you find better words to express yourself.
Something better than Email, but not so distracting as Slack.
The winner came out to be Discourse by Jeff Atwood and his team(edit: not Discord, another chat app).&lt;/p&gt;

&lt;p&gt;Having run it for over two months, we couldn’t have been happier.
Discourse sits somewhere between emails and real-time chat systems.
You feel like you’re having a real-time conversation.
But it won’t interrupt you in the middle of a 3-hour block of time you’ve set for distraction-free coding.
Discourse has become such an integral part of our company that we abandoned making decisions in meetings entirely.
All decisions happen over discourse, so everyone has a chance to voice their opinions and suggestions at their pace.
And others can embark on fact-finding and put figures together to support or deny arguments.
I’ve seen a lot smarter discussions, and hence decisions happening over our Discourse forum than over any other medium.&lt;/p&gt;

&lt;p&gt;Don’t get me wrong.
&lt;strong&gt;We still need Slack&lt;/strong&gt; for casual chit chat and team bonding.
But, Slack is an addiction.
You write one thing, someone replies; then someone else jumps in; and the result is an hour-long frenzy, where everyone feels like they’ve contributed a lot.
But a few hours later, you can’t pinpoint what changed.&lt;/p&gt;

&lt;p&gt;Most useful discussions require a bit more time and thought.
More than when someone else is typing while you’re in the middle of your sentence.&lt;/p&gt;

&lt;p&gt;This is what lead to Wisemonk.
This bot would push the conversation away from Slack into Discourse.
And this has to happen just when the team is getting sucked into the frenzy.
&lt;strong&gt;Before it’s too late&lt;/strong&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;h1 id=&#34;how-we-built-it&#34;&gt;How we built it&lt;/h1&gt;

&lt;p&gt;&lt;em&gt;— Pawan Rawal&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Wisemonk makes use of the Slack RTM API.
We make good use of the concurrency features in Go.
For each Slack channel passed in channels flag, we initialise an instance of type Counter.
The counter keeps track of the messages and other states for a Slack channel.
Each counter has a buffered channel (called &lt;code&gt;messages&lt;/code&gt;) to which Slack messages are sent.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;cmap := make(map[string]*Counter)
for _, cid := range schannels {
  wg.Add(1)
  c := &amp;amp;Counter{channelId: cid}
  c.messages = make(chan *slack.Msg, 500)
  cmap[cid] = c
  go c.checkOrIncr(rtm, wg, memmap)
}
go listen(rtm)
wg.Wait()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The listen function which runs as a goroutine gives us access to the messages sent on Slack.
We use this library to authenticate with and get messages from Slack.
Then the listen function passes messages on the relevant Go channel.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;case *slack.MessageEvent:
  if sm, ok := msg.Data.(*slack.MessageEvent); ok {
  // Putting the message on the Counter it belongs to
  m := sm.Msg
  if c, ok := cmap[m.Channel]; ok {
    c.messages &amp;lt;- &amp;amp;m
  }
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The &lt;code&gt;checkOrIncr&lt;/code&gt; method which runs as a goroutine for every counter keeps a count of the number of messages exchanged in a given interval.
Below is the basic version of the method.
It runs a never ending &lt;code&gt;for&lt;/code&gt; loop.
Now every time a message is received on the messages channel of the counter instance, it adds the message.
NewTicker function from the time package provides us with a Ticker, which sends the time on a channel with the period specified.
So every 10 seconds we get a time value and check if the count of messages exchanged increases the &lt;code&gt;maxmsg&lt;/code&gt; that we passed as a flag.
If it does, then we send a warning to the relevant Slack channel.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (c *Counter) checkOrIncr(rtm *slack.RTM, wg sync.WaitGroup, memmap map[string]string) {
   defer wg.Done()
   ticker := time.NewTicker(time.Second * 10)
   for {
       select {
       case msg := &amp;lt;-c.messages:
           c.Increment(msg, memmap)
       case &amp;lt;-ticker.C:
           count := c.Count()
           if count &amp;gt;= *maxmsg {
               go sendMessage(c, rtm)
           }
       }
   }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Once the basic functionality was in place, we added integration with discourse.
Wisemonk stores the last n messages exchanged, creates a discourse topic with the messages when they exceed the max count and shares the URL with us on Slack.&lt;/p&gt;

&lt;p&gt;You can create a discourse topic on demand too and get the URL for it. This makes shifting conversations to Discourse super easy.&lt;/p&gt;

&lt;p&gt;For the weekends or in the evenings when you want to have a casual chat and don’t want to be interrupted by Wisemonk, you could ask him to meditate, and it won’t disturb you for a while.&lt;/p&gt;

&lt;hr /&gt;

&lt;h1 id=&#34;wisemonk-in-action&#34;&gt;Wisemonk in Action&lt;/h1&gt;

&lt;p&gt;&lt;img src=&#34;https://open.dgraph.io/images/wisemonk-in-action.png&#34; alt=&#34;Wisemonk alerting us when we talk too much&#34; /&gt;
&lt;em&gt;Wisemonk alerting us when we talk too much&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://open.dgraph.io/images/wisemonk-creating-topic.png&#34; alt=&#34;Asking wisemonk to create a topic&#34; /&gt;
&lt;em&gt;Asking wisemonk to create a topic&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://open.dgraph.io/images/wisemonk-meditating.png&#34; alt=&#34;Asking wisemonk to meditate&#34; /&gt;
&lt;em&gt;Asking wisemonk to meditate&lt;/em&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;h1 id=&#34;parting-thoughts&#34;&gt;Parting Thoughts&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;&lt;a href=&#34;https://github.com/dgraph-io/wisemonk#install&#34;&gt;Setting up wisemonk&lt;/a&gt; is super easy and we have found that it has enhanced our productivity a lot.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;It&amp;rsquo;s open source and &lt;a href=&#34;https://github.com/dgraph-io/wisemonk&#34;&gt;available at Github&lt;/a&gt;, so feel free to contribute to it. We would love to hear about your usage of Wisemonk.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Yoda guides actions. Wisemonk guides conversations. Image courtesy: &lt;a href=&#34;http://www.wired.com/wp-content/uploads/2015/08/Yoda-featured1.jpg&#34;&gt;wired.com&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;div class=&#34;engage&#34;&gt;
  &lt;p&gt;We are building an open source, scalable and distributed graph database.&lt;/p&gt;
  &lt;table&gt;
    &lt;thead&gt;
    &lt;tr&gt;
    &lt;th&gt;&lt;/th&gt;
    &lt;th&gt;&lt;/th&gt;
    &lt;/tr&gt;
    &lt;/thead&gt;
    &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;See our live demo.&lt;/td&gt;
      &lt;td&gt;&lt;a href=&#34;https://dgraph.io&#34; target=&#34;_blank&#34;&gt;http://dgraph.io&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;We are hiring. Join us!&lt;/td&gt;
      &lt;td&gt;&lt;a href=&#34;https://dgraph.io#jobs&#34; target=&#34;_blank&#34;&gt;http://dgraph.io#jobs&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Star us on Github.&lt;/td&gt;
      &lt;td&gt;&lt;a href=&#34;https://github.com/dgraph-io/dgraph&#34; target=&#34;_blank&#34;&gt;https://github.com/dgraph-io/dgraph&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Ask us questions.&lt;/td&gt;
      &lt;td&gt;&lt;a href=&#34;https://discuss.dgraph.io&#34; target=&#34;_blank&#34;&gt;https://discuss.dgraph.io&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;/tbody&gt;
  &lt;/table&gt;
&lt;/div&gt;

</description>
    </item>
    
    <item>
      <title>Hello, World!</title>
      <link>https://open.dgraph.io/post/hello-world/</link>
      <pubDate>Mon, 18 Apr 2016 14:37:08 +1000</pubDate>
      
      <guid>https://open.dgraph.io/post/hello-world/</guid>
      <description>&lt;p&gt;&lt;strong&gt;I&amp;rsquo;m very excited&lt;/strong&gt; to use this first post to talk about Dgraph, what it is and why it was created.&lt;/p&gt;

&lt;p&gt;Before I explain what&amp;rsquo;s Dgraph, let&amp;rsquo;s start with a basic understanding of graphs.
A graph is a mathematical structure used to model a pairwise relationship between entities.
A graph is thus composed of nodes connected by edges.
Each node represents an entity (a person, place, thing, etc.), and each edge represents the relationship between two nodes.
Some popular graphs that we all know about are the &lt;a href=&#34;https://en.wikipedia.org/wiki/Social_graph&#34;&gt;Facebook Social Graph&lt;/a&gt; and the &lt;a href=&#34;https://en.wikipedia.org/wiki/Knowledge_Graph&#34;&gt;Google Knowledge Graph&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;A graph database is a database that uses graph structures with nodes and edges to represent, store and serve data.&lt;/p&gt;

&lt;p&gt;But who really uses graph databases? More teams and companies than you&amp;rsquo;d think.
Google, Facebook, Twitter, eBay, LinkedIn, Amazon, Dropbox, Pinterest &amp;ndash; pick a company you are familiar with.
If they&amp;rsquo;re doing something smart, chances are they&amp;rsquo;re probably using a graph database.
Even very simple web apps have much to gain from graph databases.
In the past, I&amp;rsquo;ve built a graph based REST framework and using that &lt;a href=&#34;https://mrjn.xyz/post/Porting-To-Gocrud/&#34;&gt;cut down the code in half&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;So now that we understand graphs, let&amp;rsquo;s talk about Dgraph.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Dgraph is an open source, low-latency, high throughput, native and distributed graph database.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;To understand why it was created, let&amp;rsquo;s rewind a few years back to 2011.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;strong&gt;I&amp;rsquo;d been at Google&lt;/strong&gt; about 4+ years with the Web Search Infrastructure Group.
Google had just then acquired Metaweb a year earlier in 2010.
I&amp;rsquo;d been wrapping my head around the newly acquired Knowledge Graph, trying to find ways to integrate Knowledge Graph with Google Search.
This is when I found a problem.&lt;/p&gt;

&lt;p&gt;At Google, we had multiple knowledge bearing feeds called One Boxes.
You know, the boxed snippets that sometimes show up at the top of the search results, for instance when you search for &lt;a href=&#34;https://www.google.com/#q=tesla+stock&#34;&gt;Tesla stock&lt;/a&gt;, &lt;a href=&#34;https://www.google.com/#q=weather+in+sydney&#34;&gt;Weather in Sydney&lt;/a&gt;, or &lt;a href=&#34;https://www.google.com/#q=events+in+san+francisco&#34;&gt;Events in San Francisco&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;There were multiple custom built backends, each serving a One Box.
A search query hitting &lt;em&gt;&lt;a href=&#34;https://www.google.com&#34;&gt;www.google.com&lt;/a&gt;&lt;/em&gt; would be sent iteratively through each of these One Box backends to check if any of them has a response.
When one of the backends responds, the One Box data is retrieved and rendered on the top of the search results page.
This is how that well-formatted box with just the right information shows up below the search bar, thus saving you a few clicks.&lt;/p&gt;

&lt;p&gt;As good as it sounded, One Boxes had several inefficiencies and missed opportunities.&lt;/p&gt;

&lt;p&gt;For starters, each One Box was custom built by a separate team that was responsible for running and maintaining it.
As a result, there was no particular sharing of the framework used to build the One Box.&lt;/p&gt;

&lt;p&gt;This also meant that there was no single standard for the data format used by the One Boxes.
Each One Box kept its data in its very own data structure, and no common querying language.
Thus, there didn&amp;rsquo;t exist an opportunity to share data amongst the boxes, to respond to more interesting queries that required an intersection of diverse data feeds.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;A good example of this would be the ability to &lt;em&gt;recommend events based on the weather&lt;/em&gt; to a tourist exploring NYC &amp;ndash; that couldn&amp;rsquo;t easily be done with the existing system.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;img src=&#34;https://open.dgraph.io/images/nyc.jpg&#34; alt=&#34;NYC in rain&#34; /&gt;
&lt;em&gt;Courtesy: &lt;a href=&#34;https://flic.kr/p/azztBd&#34;&gt;Several Seconds&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;This motivated me to start a project to standardize the data structures and eventually serve them all using a single backend.
Using the vast expertise of Metaweb team, we chose a data normalization structure that was also used by Knowledge Graph, the RDF Triples.
By reconciling all the various entities from the different data feeds, we could start to reuse the data.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;But, there was a second and more challenging part to the problem.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;It was to build a system that could serve structured queries with data updating in real time.
The system had to run behind Web Search, which meant that if it doesn&amp;rsquo;t respond within allocated milliseconds, Search would time out and move on.
Also, this system had to tackle a major chunk of query load to Web Search, which amounts to thousands of queries per second.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;We basically had to build a low latency, high throughput system to serve graph queries.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;It was certainly an exciting project and held much promise.
But, the harsh realities of the business environment and the attendant politics resulted in the cancellation of the project.
Shortly thereafter I left Google in 2013 and didn&amp;rsquo;t give much thought to the project.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;strong&gt;Fast forward two years,&lt;/strong&gt; I was hanging out on the Go language&amp;rsquo;s Slack channel and Stack Overflow.
I saw quite a few people complaining about a popular graph database&amp;rsquo;s performance and stability.&lt;/p&gt;

&lt;p&gt;That&amp;rsquo;s when I realized that graph databases were starting to be used more frequently than it would appear from the surface.
But a bit more digging around revealed a deeper problem.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Existing native graph databases weren&amp;rsquo;t designed to be performant or distributed.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;The ones that sharded the data and distributed it across a cluster weren&amp;rsquo;t actually native graph databases.
They were largely serving as a graph layer over another database.
This meant having many network calls should the intermediate number of results be large, which leads to performance degradation.&lt;/p&gt;

&lt;p&gt;For example, say you wanted to find &lt;strong&gt;[People living in SF who eat Sushi]&lt;/strong&gt;.
Assuming you have this data (&lt;em&gt;hey Facebook!&lt;/em&gt;) and keeping things simple, this requires 2 steps.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://open.dgraph.io/images/sushi.jpg&#34; alt=&#34;Sushi&#34; /&gt;
&lt;em&gt;Courtesy: &lt;a href=&#34;https://flic.kr/p/nLkbkQ&#34;&gt;Yannig Van de Wouwer&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;First, you find all the people living in SF, and then secondly, intersect that list with all the people who eat Sushi.&lt;/p&gt;

&lt;p&gt;As you can imagine, the intermediate step here has a &lt;em&gt;large fan-out&lt;/em&gt;, i.e. there&amp;rsquo;re over a million results.
If you were to shard the data by &lt;em&gt;entities&lt;/em&gt; (people), you&amp;rsquo;d end up broadcasting to all the servers in the cluster.
Thus, this query would be affected by even a single slow machine in the cluster.&lt;/p&gt;

&lt;p&gt;Do that for every query, and it would spike the 95%-ile latency numbers up dramatically, &lt;em&gt;higher latency being worse&lt;/em&gt;.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;strong&gt;Dgraph, on the other hand, is a native graph database&lt;/strong&gt; in the sense that the data is handled directly by Dgraph, and not given off to another database layer.&lt;/p&gt;

&lt;p&gt;This allows us to shard and relocate the data better, to minimize the number of network calls required per query.
In fact, the above query would run in 2 network calls, irrespective of the cluster size.&lt;/p&gt;

&lt;p&gt;The number of network calls being &lt;em&gt;directly proportional to the complexity of the query&lt;/em&gt;, not the number of intermediate or final results.&lt;/p&gt;

&lt;p&gt;Dgraph is designed to easily scale from meeting the needs of a small startup to that of Dropbox, or even Facebook.
This means being able to run on a laptop as well as on a big cluster of hundreds of machines serving thousands of queries per second.&lt;/p&gt;

&lt;p&gt;Additionally, it would also have to survive machine failures and partial data center collapses.
The data stored would have to be automatically replicated with no single point of failure, and be able to move around the cluster to better distribute traffic.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;This is the big vision which led to Dgraph.&lt;/strong&gt; And I&amp;rsquo;m &lt;a href=&#34;http://dgraph.io/&#34;&gt;fortunate to have a team&lt;/a&gt; that believes in and shares this vision with me.&lt;/p&gt;

&lt;p&gt;Apart from use with diverse social and knowledge graphs, Dgraph can also be used to:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;build real-time recommendation engines,&lt;/li&gt;
&lt;li&gt;do semantic search,&lt;/li&gt;
&lt;li&gt;pattern matching,&lt;/li&gt;
&lt;li&gt;serve relationship data, and&lt;/li&gt;
&lt;li&gt;serve web apps via &lt;a href=&#34;https://facebook.github.io/graphql/&#34;&gt;GraphQL&lt;/a&gt;, a full feature graph query language by Facebook.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We&amp;rsquo;ll be reporting some performance numbers for Dgraph in our next few posts, to give you an idea of what you can expect from the system.&lt;/p&gt;

&lt;hr /&gt;

&lt;div class=&#34;engage&#34;&gt;
  &lt;p&gt;We are building an open source, scalable and distributed graph database.&lt;/p&gt;
  &lt;table&gt;
    &lt;thead&gt;
    &lt;tr&gt;
    &lt;th&gt;&lt;/th&gt;
    &lt;th&gt;&lt;/th&gt;
    &lt;/tr&gt;
    &lt;/thead&gt;
    &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;See our live demo.&lt;/td&gt;
      &lt;td&gt;&lt;a href=&#34;https://dgraph.io&#34; target=&#34;_blank&#34;&gt;http://dgraph.io&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;We are hiring. Join us!&lt;/td&gt;
      &lt;td&gt;&lt;a href=&#34;https://dgraph.io#jobs&#34; target=&#34;_blank&#34;&gt;http://dgraph.io#jobs&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Star us on Github.&lt;/td&gt;
      &lt;td&gt;&lt;a href=&#34;https://github.com/dgraph-io/dgraph&#34; target=&#34;_blank&#34;&gt;https://github.com/dgraph-io/dgraph&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Ask us questions.&lt;/td&gt;
      &lt;td&gt;&lt;a href=&#34;https://discuss.dgraph.io&#34; target=&#34;_blank&#34;&gt;https://discuss.dgraph.io&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;/tbody&gt;
  &lt;/table&gt;
&lt;/div&gt;


&lt;p&gt;&lt;em&gt;Top image: &lt;a href=&#34;http://go.nasa.gov/1VlGVXx&#34;&gt;Mars Rover Curiosity in Buckskin Selfie&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>